{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " '__globals__': [],\n",
       " '__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011',\n",
       " '__version__': '1.0',\n",
       " 'y': array([[10],\n",
       "        [10],\n",
       "        [10],\n",
       "        ...,\n",
       "        [ 9],\n",
       "        [ 9],\n",
       "        [ 9]], dtype=uint8)}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "\n",
    "data = loadmat('machine-learning-ex4/ex4/ex4data1.mat')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 400), (5000, 1))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn class label n(out of k classes) into a vector of length k where index n is \"hot\"(1) while the rest are zero, \n",
    "One-hot encode y labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 10),\n",
       " array([10], dtype=uint8),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_onehot = encoder.fit_transform(y)\n",
    "y_onehot.shape, y[0], y_onehot[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network with input layer matching the size of our instance data(400 + 1), a hidden layer with 25(+1) units and an output layer with 10 units corresponding to each class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes the gradient of the sigmoid function\n",
    "def sigmoidGradient(z):\n",
    "    return np.multiply(sigmoid(z), (1 - sigmoid(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params, \n",
    "                  input_layer_size, \n",
    "                  hidden_layer_size,\n",
    "                  num_labels,\n",
    "                  X, y, lam):\n",
    "    m = X.shape[0]\n",
    "    X = np.matrix(X) # (m, n+1)\n",
    "    y = np.matrix(y) # (m, k)\n",
    "    \n",
    "    # unroll and reshape nn_params into theta1 and theta2\n",
    "    theta1 = np.matrix(np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                                 (hidden_layer_size, (input_layer_size + 1))))\n",
    "    theta2 = np.matrix(np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                                 (num_labels, (hidden_layer_size + 1))))\n",
    "    \n",
    "    # grad values\n",
    "    theta1_grad = np.zeros(theta1.shape) # (25, n+1)\n",
    "    theta2_grad = np.zeros(theta2.shape) # (k, 25+1)\n",
    "    \n",
    "    # run feed-forward pass\n",
    "    a1 = np.insert(X, 0, 1, axis=1) # (m, n+1)\n",
    "    z2 = a1 * theta1.T\n",
    "    a2 = np.insert(sigmoid(z2), 0, 1, axis=1) # (m, 25+1)\n",
    "    z3 = a2 * theta2.T\n",
    "    h = sigmoid(z3) # (m, k)\n",
    "    \n",
    "    # compute the cost\n",
    "    term1 = np.multiply(-y, np.log(h))\n",
    "    term2 = np.multiply((1 - y), np.log(1 - h))\n",
    "    J = np.sum(term1 - term2) / m\n",
    "    \n",
    "    # add the cost regularization\n",
    "    J += (lam/(2*m)) * (np.sum(np.power(theta1[:,1:], 2)) + np.sum(np.power(theta2[:,1:], 2)))\n",
    "    \n",
    "    \n",
    "    # back-propagation (vectorized)\n",
    "    delta3 = h - y # (m, k)\n",
    "    theta2_grad = (delta3.T * a2) / m\n",
    "    \n",
    "    # (m, 25)\n",
    "    delta2 = np.multiply((delta3 * theta2[:, 1:]), sigmoidGradient(z2))\n",
    "    theta1_grad = (delta2.T * a1) / m\n",
    "    \n",
    "    # regularise the gradients\n",
    "    theta1_grad[:,1:] = theta1_grad[:,1:] + (lam/m) * theta1[:,1:]\n",
    "    theta2_grad[:,1:] = theta2_grad[:,1:] + (lam/m) * theta2[:,1:]\n",
    "    \n",
    "    # unroll the gradient matrices into single array\n",
    "    grad = np.concatenate((np.ravel(theta1_grad), np.ravel(theta2_grad)))\n",
    "    \n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "n = 400\n",
    "hidden_size = 25\n",
    "k = 10\n",
    "lam = 1\n",
    "epsilon_init = 0.12\n",
    "\n",
    "# randomly initialize a parameter array of the size of the full network's parameters\n",
    "nn_params = np.random.random(size=hidden_size * (n + 1) + k * (hidden_size + 1)) * 2 * epsilon_init - epsilon_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-205.9441712493779, (10285,))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the nnCostFunction\n",
    "J, grad = nnCostFunction(nn_params, n, hidden_size, k, X, y, lam)\n",
    "J, grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 0.33368182385038\n",
       "     jac: array([ 1.05309591e-04,  1.86369043e-06,  1.67382359e-06, ...,\n",
       "        1.80675175e-04, -1.80303129e-04, -5.39350949e-05])\n",
       " message: 'Max. number of function evaluations reached'\n",
       "    nfev: 250\n",
       "     nit: 20\n",
       "  status: 3\n",
       " success: False\n",
       "       x: array([-2.00382484,  0.00931845,  0.00836912, ..., -0.39113819,\n",
       "        0.2902884 ,  2.0402121 ])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the network\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "fmin = minimize(fun=nnCostFunction, x0=nn_params, args=(n, hidden_size, k, X, y_onehot, lam), \n",
    "               method='TNC', jac=True, options={'maxiter': 250})\n",
    "fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10],\n",
       "       [10],\n",
       "       [10],\n",
       "       ...,\n",
       "       [ 9],\n",
       "       [ 9],\n",
       "       [ 9]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.matrix(X)\n",
    "theta1 = np.matrix(np.reshape(fmin.x[:hidden_size * (n + 1)],\n",
    "                             (hidden_size, (n + 1))))\n",
    "theta2 = np.matrix(np.reshape(fmin.x[(hidden_size * (n + 1)):],\n",
    "                             (k, (hidden_size + 1))))\n",
    "\n",
    "# run feed-forward pass\n",
    "a1 = np.insert(X, 0, 1, axis=1) # (m, n+1)\n",
    "z2 = a1 * theta1.T\n",
    "a2 = np.insert(sigmoid(z2), 0, 1, axis=1) # (m, 25+1)\n",
    "z3 = a2 * theta2.T\n",
    "h = sigmoid(z3)\n",
    "\n",
    "y_pred = np.array(np.argmax(h, axis=1) + 1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.28"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = [1 if a == b else 0 for (a, b) in zip(y_pred, y)]\n",
    "accuracy = sum(correct) / len(correct)\n",
    "(accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
